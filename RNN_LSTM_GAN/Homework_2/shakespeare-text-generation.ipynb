{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Shakespearean Text with Character Based RNNs\n",
    "\n",
    "Problem Statement: Given a character or sequence of characters, we want to predict the next character at each time step. Model is trained to follow a language similar to the works of Shakespeare. The tinyshakespear dataset is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "#check if decoding is needed: text may need to be decoded as utf-8\n",
    "text = open('./shakespeare_train.txt', 'r').read() \n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique characters: 67\n"
     ]
    }
   ],
   "source": [
    "#Find Vocabulary (set of characters)\n",
    "vocabulary = sorted(set(text))\n",
    "print('No. of unique characters: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#character to index mapping\n",
    "char2index = {c:i for i,c in enumerate(vocabulary)}\n",
    "int_text = np.array([char2index[i] for i in text])\n",
    "\n",
    "#Index to character mapping\n",
    "index2char = np.array(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to Index: \n",
      "\n",
      "  '\\n':   0\n",
      "  ' ' :   1\n",
      "  '!' :   2\n",
      "  '$' :   3\n",
      "  '&' :   4\n",
      "  \"'\" :   5\n",
      "  ',' :   6\n",
      "  '-' :   7\n",
      "  '.' :   8\n",
      "  '3' :   9\n",
      "  ':' :  10\n",
      "  ';' :  11\n",
      "  '?' :  12\n",
      "  'A' :  13\n",
      "  'B' :  14\n",
      "  'C' :  15\n",
      "  'D' :  16\n",
      "  'E' :  17\n",
      "  'F' :  18\n",
      "  'G' :  19\n",
      "  'H' :  20\n",
      "  'I' :  21\n",
      "  'J' :  22\n",
      "  'K' :  23\n",
      "  'L' :  24\n",
      "  'M' :  25\n",
      "  'N' :  26\n",
      "  'O' :  27\n",
      "  'P' :  28\n",
      "  'Q' :  29\n",
      "  'R' :  30\n",
      "  'S' :  31\n",
      "  'T' :  32\n",
      "  'U' :  33\n",
      "  'V' :  34\n",
      "  'W' :  35\n",
      "  'X' :  36\n",
      "  'Y' :  37\n",
      "  'Z' :  38\n",
      "  '[' :  39\n",
      "  ']' :  40\n",
      "  'a' :  41\n",
      "  'b' :  42\n",
      "  'c' :  43\n",
      "  'd' :  44\n",
      "  'e' :  45\n",
      "  'f' :  46\n",
      "  'g' :  47\n",
      "  'h' :  48\n",
      "  'i' :  49\n",
      "  'j' :  50\n",
      "  'k' :  51\n",
      "  'l' :  52\n",
      "  'm' :  53\n",
      "  'n' :  54\n",
      "  'o' :  55\n",
      "  'p' :  56\n",
      "  'q' :  57\n",
      "  'r' :  58\n",
      "  's' :  59\n",
      "  't' :  60\n",
      "  'u' :  61\n",
      "  'v' :  62\n",
      "  'w' :  63\n",
      "  'x' :  64\n",
      "\n",
      "Input text to Integer: \n",
      "\n",
      "'First Citizen:\\nBefor' mapped to [18 49 58 59 60  1 15 49 60 49 66 45 54 10  0 14 45 46 55 58]\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "print(\"Character to Index: \\n\")\n",
    "for char,_ in zip(char2index, range(65)):\n",
    "    print('  {:4s}: {:3d}'.format(repr(char), char2index[char]))\n",
    "\n",
    "print(\"\\nInput text to Integer: \\n\")\n",
    "print('{} mapped to {}'.format(repr(text[:20]),int_text[:20])) #use repr() for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length= 150 #max number of characters that can be fed as a single input\n",
    "examples_per_epoch = len(text)\n",
    "\n",
    "#converts text (vector) into character index stream\n",
    "#Reference: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(int_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sequences from the individual characters. Our required size will be seq_length + 1 (character RNN)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Stream: \n",
      "\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "\n",
      "Sequence: \n",
      "\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\n",
      "\"l:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
      "\"ill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good \"\n",
      "'citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but '\n",
      "'the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the '\n",
      "'object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we '\n",
      "'become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caiu'\n",
      "\"s Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst \"\n",
      "'Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not m'\n",
      "'aliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was'\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "print(\"Character Stream: \\n\")\n",
    "for i in char_dataset.take(10):\n",
    "    print(index2char[i.numpy()])  \n",
    "\n",
    "print(\"\\nSequence: \\n\")\n",
    "for i in sequences.take(10):\n",
    "    print(repr(''.join(index2char[i.numpy()])))  #use repr() for more clarity. str() keeps formatting it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Target value: for each sequence of characters, we return that sequence, shifted one position to the right, along with the new character that is predicted to follow the sequence.\n",
    "\n",
    "To create training examples of (input, target) pairs, we take the given sequence. The input is sequence with last word removed. Target is sequence with first word removed. Example: sequence: abc d ef input: abc d e target: bc d ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_target_pair(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(create_input_target_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nA'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(index2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(index2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating batches\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer used to shuffle the dataset \n",
    "# Reference: https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 256\n",
    "rnn_units= 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Layers used:\n",
    "\n",
    "Input Layer: Maps character to 256 dimension vector\n",
    "\n",
    "GRU Layer: RNN of size 1024\n",
    "\n",
    "Dense Layer: Output with same size as vocabulary\n",
    "\n",
    "Since it is a character level RNN, we can use keras.Sequential model (All layers have single input and single output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units, \n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "# Reference for theory: https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = build_model_lstm(vocab_size = vocab_size,\n",
    "                              embedding_dim=embedding_dim,\n",
    "                              rnn_units=rnn_units, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing: shape\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_prediction = lstm_model(input_example_batch)\n",
    "    assert (example_prediction.shape == (BATCH_SIZE, seq_length, vocab_size)), \"Shape error\"\n",
    "    #print(example_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           17152     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 67)            68675     \n",
      "=================================================================\n",
      "Total params: 5,332,803\n",
      "Trainable params: 5,332,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary() \n",
    "#check shapes if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 150, 67)\n",
      "Loss:       4.2052507\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "#Loss Function reference: https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/\n",
    "\n",
    "example_loss  = loss(target_example_batch, example_prediction)\n",
    "print(\"Prediction shape: \", example_prediction.shape)\n",
    "print(\"Loss:      \", example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dir_checkpoints= './training_checkpoints_LSTM'\n",
    "checkpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") #name\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=60 #increase number of epochs for better results (lesser loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "450/450 [==============================] - 89s 191ms/step - loss: 2.5687\n",
      "Epoch 2/60\n",
      "450/450 [==============================] - 89s 195ms/step - loss: 1.5557\n",
      "Epoch 3/60\n",
      "450/450 [==============================] - 90s 198ms/step - loss: 1.3790\n",
      "Epoch 4/60\n",
      "450/450 [==============================] - 91s 200ms/step - loss: 1.3067\n",
      "Epoch 5/60\n",
      "450/450 [==============================] - 92s 201ms/step - loss: 1.2618\n",
      "Epoch 6/60\n",
      "450/450 [==============================] - 92s 201ms/step - loss: 1.2296\n",
      "Epoch 7/60\n",
      " 82/450 [====>.........................] - ETA: 1:14 - loss: 1.2034"
     ]
    }
   ],
   "source": [
    "history = lstm_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(lstm_dir_checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "lstm_model.load_weights(tf.train.latest_checkpoint(lstm_dir_checkpoints))\n",
    "lstm_model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    num_generate = 1000 #Number of characters to be generated\n",
    "\n",
    "    input_eval = [char2index[s] for s in start_string] #vectorising input\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 0.5\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(index2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "#print(generate_text(lstm_model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction with User Input\n",
    "lstm_test = input(\"Enter your starting string: \")\n",
    "print(generate_text(lstm_model, start_string=lstm_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
