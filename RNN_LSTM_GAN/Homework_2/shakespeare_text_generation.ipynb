{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "shakespeare-text-generation.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsGMytxv-hzK"
      },
      "source": [
        "# Generating Shakespearean Text with Character Based RNNs\n",
        "\n",
        "Problem Statement: Given a character or sequence of characters, we want to predict the next character at each time step. Model is trained to follow a language similar to the works of Shakespeare. The tinyshakespear dataset is used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "R3TTZJUh-hzQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP5bD5GjDnFq"
      },
      "source": [
        "def read_text(URL):\n",
        "    with io.open(URL, 'r', encoding='utf8') as f:\n",
        "        text = f.read()\n",
        "    # Character's collection\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4GFEBFq-hzT",
        "outputId": "cc51aa3c-8adf-4edb-8df4-966880c5d5dc"
      },
      "source": [
        "#check if decoding is needed: text may need to be decoded as utf-8\n",
        "text = open('./shakespeare_train.txt', 'r').read()\n",
        "print(text[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D-U40VJ-hzY",
        "outputId": "9b344c75-90ed-467e-d784-c526f4c9d660"
      },
      "source": [
        "#Find Vocabulary (set of characters)\n",
        "vocabulary = sorted(set(text))\n",
        "print('No. of unique characters: {}'.format(len(vocabulary)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of unique characters: 67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RawWO8go-hza"
      },
      "source": [
        "## Preprocessing Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIMJVhs8-hzb"
      },
      "source": [
        "#character to index mapping\n",
        "char2index = {c:i for i,c in enumerate(vocabulary)}\n",
        "int_text = np.array([char2index[i] for i in text])\n",
        "\n",
        "#Index to character mapping\n",
        "index2char = np.array(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW1legLa-hzc",
        "outputId": "f9b028ec-ace5-4d70-f111-40c49cf7c493"
      },
      "source": [
        "#Testing\n",
        "print(\"Character to Index: \\n\")\n",
        "for char,_ in zip(char2index, range(65)):\n",
        "    print('  {:4s}: {:3d}'.format(repr(char), char2index[char]))\n",
        "\n",
        "print(\"\\nInput text to Integer: \\n\")\n",
        "print('{} mapped to {}'.format(repr(text[:20]),int_text[:20])) #use repr() for debugging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Character to Index: \n",
            "\n",
            "  '\\n':   0\n",
            "  ' ' :   1\n",
            "  '!' :   2\n",
            "  '$' :   3\n",
            "  '&' :   4\n",
            "  \"'\" :   5\n",
            "  ',' :   6\n",
            "  '-' :   7\n",
            "  '.' :   8\n",
            "  '3' :   9\n",
            "  ':' :  10\n",
            "  ';' :  11\n",
            "  '?' :  12\n",
            "  'A' :  13\n",
            "  'B' :  14\n",
            "  'C' :  15\n",
            "  'D' :  16\n",
            "  'E' :  17\n",
            "  'F' :  18\n",
            "  'G' :  19\n",
            "  'H' :  20\n",
            "  'I' :  21\n",
            "  'J' :  22\n",
            "  'K' :  23\n",
            "  'L' :  24\n",
            "  'M' :  25\n",
            "  'N' :  26\n",
            "  'O' :  27\n",
            "  'P' :  28\n",
            "  'Q' :  29\n",
            "  'R' :  30\n",
            "  'S' :  31\n",
            "  'T' :  32\n",
            "  'U' :  33\n",
            "  'V' :  34\n",
            "  'W' :  35\n",
            "  'X' :  36\n",
            "  'Y' :  37\n",
            "  'Z' :  38\n",
            "  '[' :  39\n",
            "  ']' :  40\n",
            "  'a' :  41\n",
            "  'b' :  42\n",
            "  'c' :  43\n",
            "  'd' :  44\n",
            "  'e' :  45\n",
            "  'f' :  46\n",
            "  'g' :  47\n",
            "  'h' :  48\n",
            "  'i' :  49\n",
            "  'j' :  50\n",
            "  'k' :  51\n",
            "  'l' :  52\n",
            "  'm' :  53\n",
            "  'n' :  54\n",
            "  'o' :  55\n",
            "  'p' :  56\n",
            "  'q' :  57\n",
            "  'r' :  58\n",
            "  's' :  59\n",
            "  't' :  60\n",
            "  'u' :  61\n",
            "  'v' :  62\n",
            "  'w' :  63\n",
            "  'x' :  64\n",
            "\n",
            "Input text to Integer: \n",
            "\n",
            "'First Citizen:\\nBefor' mapped to [18 49 58 59 60  1 15 49 60 49 66 45 54 10  0 14 45 46 55 58]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpiqSNm8-hze"
      },
      "source": [
        "## Create Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAqGDNYh-hze"
      },
      "source": [
        "seq_length= 150 #max number of characters that can be fed as a single input\n",
        "examples_per_epoch = len(text)\n",
        "\n",
        "#converts text (vector) into character index stream\n",
        "#Reference: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(int_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFP-UN2A-hzg"
      },
      "source": [
        "#Create sequences from the individual characters. Our required size will be seq_length + 1 (character RNN)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-nTZomu-hzh",
        "outputId": "edd863da-28aa-46d4-cd75-c8fc8a399eae"
      },
      "source": [
        "#Testing\n",
        "\n",
        "print(\"\\nSequence: \\n\")\n",
        "for i in sequences.take(10):\n",
        "    print(repr(''.join(index2char[i.numpy()])))  #use repr() for more clarity. str() keeps formatting it"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Character Stream: \n",
            "\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "\n",
            "Sequence: \n",
            "\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\n",
            "\"l:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
            "\"ill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good \"\n",
            "'citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but '\n",
            "'the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the '\n",
            "'object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we '\n",
            "'become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caiu'\n",
            "\"s Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst \"\n",
            "'Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not m'\n",
            "'aliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iseD0sEs-hzj"
      },
      "source": [
        "\n",
        "Target value: for each sequence of characters, we return that sequence, shifted one position to the right, along with the new character that is predicted to follow the sequence.\n",
        "\n",
        "To create training examples of (input, target) pairs, we take the given sequence. The input is sequence with last word removed. Target is sequence with first word removed. Example: sequence: abc d ef input: abc d e target: bc d ef"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwT_x--V-hzk"
      },
      "source": [
        "def create_input_target_pair(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(create_input_target_pair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6x3NgZ4-hzl",
        "outputId": "f4c4509d-624f-4b58-c0f5-3d294d616a64"
      },
      "source": [
        "#Testing\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(index2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(index2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nA'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlG-G_4l-hzm",
        "outputId": "a8db3048-a8d9-49dc-bad7-95ecb5ae996a"
      },
      "source": [
        "#Creating batches\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer used to shuffle the dataset \n",
        "# Reference: https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9xIjmcf-hzo"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFeoUn4h-hzo"
      },
      "source": [
        "vocab_size = len(vocabulary)\n",
        "embedding_dim = 256\n",
        "rnn_units= 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOLYXmnw-hzp"
      },
      "source": [
        "3 Layers used:\n",
        "\n",
        "Input Layer: Maps character to 256 dimension vector\n",
        "\n",
        "GRU Layer: LSTM of size 1024\n",
        "\n",
        "Dense Layer: Output with same size as vocabulary\n",
        "\n",
        "Since it is a character level RNN, we can use keras.Sequential model (All layers have single input and single output)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOUqJvhR-hzq"
      },
      "source": [
        "# Reference for theory: https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/\n",
        "\n",
        "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units, \n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqePWfbF-hzq"
      },
      "source": [
        "lstm_model = build_model_lstm(vocab_size = vocab_size,\n",
        "                              embedding_dim=embedding_dim,\n",
        "                              rnn_units=rnn_units, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EZBRsbI-hzr"
      },
      "source": [
        "#Testing: shape\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_prediction = lstm_model(input_example_batch)\n",
        "    assert (example_prediction.shape == (BATCH_SIZE, seq_length, vocab_size)), \"Shape error\"\n",
        "    #print(example_prediction.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMIFtoCB-hzs",
        "outputId": "340707fe-aebb-4ae4-812e-02c9e5e4eb23"
      },
      "source": [
        "lstm_model.summary() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           17152     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 67)            68675     \n",
            "=================================================================\n",
            "Total params: 5,332,803\n",
            "Trainable params: 5,332,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sroXCrWB-hzt"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-ijC_j3-hzt"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S3Htjq1dDBu"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH_zSYg1-hzu",
        "outputId": "a58850fc-d2c0-4360-df08-7f4fb10e8f34"
      },
      "source": [
        "example_loss  = loss(target_example_batch, example_prediction)\n",
        "print(\"Prediction shape: \", example_prediction.shape)\n",
        "print(\"Loss:      \", example_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 150, 67)\n",
            "Loss:       4.205296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pul7_S5K-hzu"
      },
      "source": [
        "lstm_model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAG6HTk0-hzv"
      },
      "source": [
        "lstm_dir_checkpoints= './training_checkpoints_LSTM'\n",
        "checkpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") #name\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xlfrfjf-hzv"
      },
      "source": [
        "EPOCHS=60 #increase number of epochs for better results (lesser loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt-BZRCR-hzv",
        "outputId": "c645eee4-8efc-47d2-da79-92766bd549ec"
      },
      "source": [
        "history = lstm_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "450/450 [==============================] - 48s 99ms/step - loss: 2.1588\n",
            "Epoch 2/60\n",
            "450/450 [==============================] - 47s 101ms/step - loss: 1.5349\n",
            "Epoch 3/60\n",
            "450/450 [==============================] - 48s 103ms/step - loss: 1.3816\n",
            "Epoch 4/60\n",
            "450/450 [==============================] - 48s 104ms/step - loss: 1.3147\n",
            "Epoch 5/60\n",
            "450/450 [==============================] - 48s 104ms/step - loss: 1.2720\n",
            "Epoch 6/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 1.2412\n",
            "Epoch 7/60\n",
            "450/450 [==============================] - 50s 108ms/step - loss: 1.2150\n",
            "Epoch 8/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 1.1926\n",
            "Epoch 9/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 1.1713\n",
            "Epoch 10/60\n",
            "450/450 [==============================] - 50s 108ms/step - loss: 1.1515\n",
            "Epoch 11/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 1.1320\n",
            "Epoch 12/60\n",
            "450/450 [==============================] - 50s 108ms/step - loss: 1.1126\n",
            "Epoch 13/60\n",
            "450/450 [==============================] - 51s 110ms/step - loss: 1.0935\n",
            "Epoch 14/60\n",
            "450/450 [==============================] - 49s 107ms/step - loss: 1.0749\n",
            "Epoch 15/60\n",
            "450/450 [==============================] - 51s 110ms/step - loss: 1.0567\n",
            "Epoch 16/60\n",
            "450/450 [==============================] - 51s 110ms/step - loss: 1.0409\n",
            "Epoch 17/60\n",
            "450/450 [==============================] - 51s 110ms/step - loss: 1.0245\n",
            "Epoch 18/60\n",
            "450/450 [==============================] - 51s 110ms/step - loss: 1.0078\n",
            "Epoch 19/60\n",
            "450/450 [==============================] - 51s 111ms/step - loss: 0.9950\n",
            "Epoch 20/60\n",
            "450/450 [==============================] - 50s 108ms/step - loss: 0.9802\n",
            "Epoch 21/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.9671\n",
            "Epoch 22/60\n",
            "450/450 [==============================] - 50s 108ms/step - loss: 0.9557\n",
            "Epoch 23/60\n",
            "450/450 [==============================] - 51s 110ms/step - loss: 0.9460\n",
            "Epoch 24/60\n",
            "450/450 [==============================] - 49s 107ms/step - loss: 0.9375\n",
            "Epoch 25/60\n",
            "450/450 [==============================] - 50s 109ms/step - loss: 0.9314\n",
            "Epoch 26/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.9201\n",
            "Epoch 27/60\n",
            "450/450 [==============================] - 51s 109ms/step - loss: 0.9129\n",
            "Epoch 28/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.9074\n",
            "Epoch 29/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.9019\n",
            "Epoch 30/60\n",
            "450/450 [==============================] - 50s 108ms/step - loss: 0.8968\n",
            "Epoch 31/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.8926\n",
            "Epoch 32/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8899\n",
            "Epoch 33/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8853\n",
            "Epoch 34/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8828\n",
            "Epoch 35/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8792\n",
            "Epoch 36/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8807\n",
            "Epoch 37/60\n",
            "450/450 [==============================] - 50s 108ms/step - loss: 0.8773\n",
            "Epoch 38/60\n",
            "450/450 [==============================] - 51s 110ms/step - loss: 0.8768\n",
            "Epoch 39/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.8765\n",
            "Epoch 40/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.8737\n",
            "Epoch 41/60\n",
            "450/450 [==============================] - 50s 108ms/step - loss: 0.8733\n",
            "Epoch 42/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.8762\n",
            "Epoch 43/60\n",
            "450/450 [==============================] - 50s 109ms/step - loss: 0.8821\n",
            "Epoch 44/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.8746\n",
            "Epoch 45/60\n",
            "450/450 [==============================] - 50s 109ms/step - loss: 0.8743\n",
            "Epoch 46/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.8756\n",
            "Epoch 47/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.8803\n",
            "Epoch 48/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8889\n",
            "Epoch 49/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8761\n",
            "Epoch 50/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8766\n",
            "Epoch 51/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8888\n",
            "Epoch 52/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8822\n",
            "Epoch 53/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8813\n",
            "Epoch 54/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8833\n",
            "Epoch 55/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8855\n",
            "Epoch 56/60\n",
            "450/450 [==============================] - 49s 106ms/step - loss: 0.8957\n",
            "Epoch 57/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8905\n",
            "Epoch 58/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8919\n",
            "Epoch 59/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.9025\n",
            "Epoch 60/60\n",
            "450/450 [==============================] - 49s 105ms/step - loss: 0.8977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TYCfvgmC-hzw",
        "outputId": "a9f79a9a-29ba-4b86-b881-0236b9076a56"
      },
      "source": [
        "tf.train.latest_checkpoint(lstm_dir_checkpoints)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints_LSTM/checkpt_60'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0zZD4z9-hzx"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhyHmBdK-hzx",
        "outputId": "6a14f971-361b-4ba8-adb2-cb6f886b64cb"
      },
      "source": [
        "lstm_model = build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "lstm_model.load_weights(tf.train.latest_checkpoint(lstm_dir_checkpoints))\n",
        "lstm_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "lstm_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            17152     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 67)             68675     \n",
            "=================================================================\n",
            "Total params: 5,332,803\n",
            "Trainable params: 5,332,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk5vi_Ui-hzy"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    num_generate = 1000 #Number of characters to be generated\n",
        "\n",
        "    input_eval = [char2index[s] for s in start_string] #vectorising input\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.5\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VveBG2f2-hzz",
        "outputId": "6d0abe6c-7963-46de-ec36-d8e036b97adf"
      },
      "source": [
        "#Prediction with User Input\n",
        "lstm_test = input(\"Enter your starting string: \")\n",
        "print(generate_text(lstm_model, start_string=lstm_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your starting string: Fisrt citizen\n",
            "Fisrt citizens; and\n",
            "in the carpet comes so grievously done, as it is an\n",
            "old man and a robber, and that sterile in me\n",
            "doth live and to be the other earnest.\n",
            "\n",
            "All Servants:\n",
            "Ay, sir.\n",
            "\n",
            "FALSTAFF:\n",
            "What sayest thou, thine elder thou? What's thyself?\n",
            "\n",
            "Third Servingman:\n",
            "But what said she?\n",
            "\n",
            "Servant:\n",
            "Sir, I will seek him to the senators.\n",
            "\n",
            "ACHILLES:\n",
            "Go to him.\n",
            "\n",
            "TRANIO:\n",
            "A most conscience swell that are not such a natural;\n",
            "Yield us the superfluous castle of the time.\n",
            "\n",
            "FLAVIUS:\n",
            "Away, away, away!\n",
            "\n",
            "CASSANDRA:\n",
            "\n",
            "HAMLET:\n",
            "Then in some return there is no such sting.\n",
            "\n",
            "HORATIO:\n",
            "In the sea shall answer the best of your daughter,\n",
            "And you shall find me praise unto the king.\n",
            "\n",
            "GRATIANO:\n",
            "What peer the youth of men and motions?\n",
            "O you pretty of this great company!\n",
            "For my particular arise, obier, all false\n",
            "As to the enterprise of ill restraint,\n",
            "And let him stand in present sickness and the state\n",
            "For that which he conjuncts at fear and dropp'd,\n",
            "To the great lips, set down his way in bed,\n",
            "Ungive the lean and sickness fill\n",
            "When Helen\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}